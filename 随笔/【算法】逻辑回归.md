## 使用对数几率的好处（原因）
直接对分类的概率建模，无需实现假设数据分布，从而避免了假设分布不准确带来的问题；
不仅可预测出类别，还能得到该预测的概率，这对一些利用概率辅助决策的任务很有用；
对数几率函数是任意阶可导的凸函数，有许多数值优化算法都可以求出最优解。

## 逻辑回归与线性回归的区别
1、逻辑回归是分类模型
2、逻辑回归在0附近才敏感，线性回归的敏感度一致

```
逻辑回归是在线性回归的基础上加了一个 Sigmoid 函数（非线形）映射，使得逻辑回归称为了一个优秀的分类算法。本质上来说，两者都属于广义线性模型，但他们两个要解决的问题不一样，逻辑回归解决的是分类问题，输出的是离散值，线性回归解决的是回归问题，输出的连续值。
我们需要明确 Sigmoid 函数到底起了什么作用：
线性回归是在实数域范围内进行预测，而分类范围则需要在 [0,1]，逻辑回归减少了预测范围；
线性回归在实数域上敏感度一致，而逻辑回归在 0 附近敏感，在远离 0 点位置不敏感，这个的好处就是模型更加关注分类边界，可以增加模型的鲁棒性。
```

## 逻辑回归做 特征离散化
1、提高特征的稳定性
2、提高模型的表达能力
3、部分的交叉特征
```
我们在使用逻辑回归的时候很少会把数据直接丢给 LR 来训练，我们一般会对特征进行离散化处理，这样做的优势大致有以下几点：
离散后稀疏向量内积乘法运算速度更快，计算结果也方便存储，容易扩展；
离散后的特征对异常值更具鲁棒性，如 age>30 为 1 否则为 0，对于年龄为 200 的也不会对模型造成很大的干扰；
LR 属于广义线性模型，表达能力有限，经过离散化后，每个变量有单独的权重，这相当于引入了非线性，能够提升模型的表达能力，加大拟合；
离散后特征可以进行特征交叉，提升表达能力，由 M+N 个变量编程 M*N 个变量，进一步引入非线形，提升了表达能力；
特征离散后模型更稳定，如用户年龄区间，不会因为用户年龄长了一岁就变化；
总的来说，特征离散化以后起到了加快计算，简化模型和增加泛化能力的作用。
```

逻辑回归是先拟合决策边界，再建立与决策边界的概率联系

lr的缺点是对特征的拟合能力较弱，需要利用xgboost 和 FM因子分解机进行特征增广；

[lr的参考文章](https://zhuanlan.zhihu.com/p/100763009https://zhuanlan.zhihu.com/p/100763009)



真的需要对日常的东西进行及时的梳理和归档了。
[xgboost](https://zhuanlan.zhihu.com/p/148050748)

[xgboost支持自定义损失函数](https://zhuanlan.zhihu.com/p/96899266)
[xgboost 面试](https://zhuanlan.zhihu.com/p/56175215)
[在推荐领域内的LR融合模型](https://www.zhihu.com/question/62109451/answer/194955304)
[FM因子分解算法](https://www.zhihu.com/search?type=content&q=fm%E7%AE%97%E6%B3%95)